{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Stage 1\n",
      "10E\n",
      "2ED\n",
      "3ED\n",
      "4ED\n",
      "5DN\n",
      "5ED\n",
      "6ED\n",
      "7ED\n",
      "8ED\n",
      "9ED\n",
      "A25\n",
      "AER\n",
      "AKH\n",
      "ALA\n",
      "ALL\n",
      "ANA\n",
      "APC\n",
      "ARB\n",
      "ARC\n",
      "ARN\n",
      "ATH\n",
      "ATQ\n",
      "AVR\n",
      "BBD\n",
      "BFZ\n",
      "BNG\n",
      "BOK\n",
      "BRB\n",
      "BTD\n",
      "C13\n",
      "C14\n",
      "C15\n",
      "C16\n",
      "C17\n",
      "C18\n",
      "CED\n",
      "CEI\n",
      "CHK\n",
      "CHR\n",
      "CM1\n",
      "CM2\n",
      "CMA\n",
      "CMD\n",
      "CN2\n",
      "CNS\n",
      "CON\n",
      "CP1\n",
      "CP2\n",
      "CP3\n",
      "CSP\n",
      "CST\n",
      "DD1\n",
      "DD2\n",
      "DDC\n",
      "DDD\n",
      "DDE\n",
      "DDF\n",
      "DDG\n",
      "DDH\n",
      "DDI\n",
      "DDJ\n",
      "DDK\n",
      "DDL\n",
      "DDM\n",
      "DDN\n",
      "DDO\n",
      "DDP\n",
      "DDQ\n",
      "DDR\n",
      "DDS\n",
      "DDT\n",
      "DDU\n",
      "DGM\n",
      "DIS\n",
      "DKA\n",
      "DKM\n",
      "DOM\n",
      "DPA\n",
      "DRB\n",
      "DRK\n",
      "DST\n",
      "DTK\n",
      "DVD\n",
      "E01\n",
      "E02\n",
      "EMA\n",
      "EMN\n",
      "EVE\n",
      "EVG\n",
      "EXO\n",
      "EXP\n",
      "F01\n",
      "F02\n",
      "F03\n",
      "F04\n",
      "F05\n",
      "F06\n",
      "F07\n",
      "F08\n",
      "F09\n",
      "F10\n",
      "F11\n",
      "F12\n",
      "F13\n",
      "F14\n",
      "F15\n",
      "F16\n",
      "F17\n",
      "F18\n",
      "FBB\n",
      "FEM\n",
      "FNM\n",
      "FRF\n",
      "FUT\n",
      "G00\n",
      "G01\n",
      "G02\n",
      "G03\n",
      "G04\n",
      "G05\n",
      "G06\n",
      "G07\n",
      "G08\n",
      "G09\n",
      "G10\n",
      "G11\n",
      "G17\n",
      "G18\n",
      "G99\n",
      "GK1\n",
      "GK2\n",
      "GNT\n",
      "GPT\n",
      "GRN\n",
      "GS1\n",
      "GTC\n",
      "GVL\n",
      "H09\n",
      "H17\n",
      "HHO\n",
      "HML\n",
      "HOP\n",
      "HOU\n",
      "HTR\n",
      "HTR17\n",
      "ICE\n",
      "IMA\n",
      "INV\n",
      "ISD\n",
      "ITP\n",
      "J12\n",
      "J13\n",
      "J14\n",
      "J15\n",
      "J16\n",
      "J17\n",
      "J18\n",
      "JGP\n",
      "JOU\n",
      "JUD\n",
      "JVC\n",
      "KLD\n",
      "KTK\n",
      "L12\n",
      "L13\n",
      "L14\n",
      "L15\n",
      "L16\n",
      "L17\n",
      "LEA\n",
      "LEB\n",
      "LEG\n",
      "LGN\n",
      "LRW\n",
      "M10\n",
      "M11\n",
      "M12\n",
      "M13\n",
      "M14\n",
      "M15\n",
      "M19\n",
      "MBS\n",
      "MD1\n",
      "ME1\n",
      "ME2\n",
      "ME3\n",
      "ME4\n",
      "MED\n",
      "MGB\n",
      "MIR\n",
      "MM2\n",
      "MM3\n",
      "MMA\n",
      "MMQ\n",
      "MOR\n",
      "MP2\n",
      "MPR\n",
      "MPS\n",
      "MRD\n",
      "NEM\n",
      "NPH\n",
      "OARC\n",
      "OC13\n",
      "OC14\n",
      "OC15\n",
      "OC16\n",
      "OC17\n",
      "OC18\n",
      "OCM1\n",
      "OCMD\n",
      "ODY\n",
      "OE01\n",
      "OGW\n",
      "OHOP\n",
      "OLGC\n",
      "ONS\n",
      "OPC2\n",
      "OPCA\n",
      "ORI\n",
      "OVNT\n",
      "P02\n",
      "P03\n",
      "P04\n",
      "P05\n",
      "P06\n",
      "P07\n",
      "P08\n",
      "P09\n",
      "P10\n",
      "P10E\n",
      "P11\n",
      "P15A\n",
      "P2HG\n",
      "PAER\n",
      "PAKH\n",
      "PAL00\n",
      "PAL01\n",
      "PAL02\n",
      "PAL03\n",
      "PAL04\n",
      "PAL05\n",
      "PAL06\n",
      "PAL99\n",
      "PALP\n",
      "PARC\n",
      "PARL\n",
      "PAVR\n",
      "PBBD\n",
      "PBFZ\n",
      "PBNG\n",
      "PBOK\n",
      "PC2\n",
      "PCA\n",
      "PCEL\n",
      "PCMD\n",
      "PCMP\n",
      "PCY\n",
      "PD2\n",
      "PD3\n",
      "PDD2\n",
      "PDGM\n",
      "PDKA\n",
      "PDOM\n",
      "PDP10\n",
      "PDP11\n",
      "PDP12\n",
      "PDP13\n",
      "PDP14\n",
      "PDRC\n",
      "PDTK\n",
      "PDTP\n",
      "PELP\n",
      "PEMN\n",
      "PF19\n",
      "PFRF\n",
      "PG07\n",
      "PG08\n",
      "PGPX\n",
      "PGRN\n",
      "PGRU\n",
      "PGTC\n",
      "PGTW\n",
      "PHEL\n",
      "PHOP\n",
      "PHOU\n",
      "PHPR\n",
      "PHUK\n",
      "PI13\n",
      "PI14\n",
      "PIDW\n",
      "PISD\n",
      "PJAS\n",
      "PJJT\n",
      "PJOU\n",
      "PJSE\n",
      "PKLD\n",
      "PKTK\n",
      "PLC\n",
      "PLGM\n",
      "PLNY\n",
      "PLPA\n",
      "PLS\n",
      "PM10\n",
      "PM11\n",
      "PM12\n",
      "PM13\n",
      "PM14\n",
      "PM15\n",
      "PM19\n",
      "PMBS\n",
      "PMEI\n",
      "PMOA\n",
      "PMPS\n",
      "PMPS06\n",
      "PMPS07\n",
      "PMPS08\n",
      "PMPS09\n",
      "PMPS10\n",
      "PMPS11\n",
      "PNAT\n",
      "PNPH\n",
      "POGW\n",
      "POR\n",
      "PORI\n",
      "PPC1\n",
      "PPOD\n",
      "PPRE\n",
      "PPRO\n",
      "PR2\n",
      "PRED\n",
      "PREL\n",
      "PRES\n",
      "PRIX\n",
      "PRM\n",
      "PRN\n",
      "PROE\n",
      "PRTR\n",
      "PRW2\n",
      "PRWK\n",
      "PS11\n",
      "PS14\n",
      "PS15\n",
      "PS16\n",
      "PS17\n",
      "PS18\n",
      "PSAL\n",
      "PSDC\n",
      "PSOI\n",
      "PSOM\n",
      "PSS1\n",
      "PSS2\n",
      "PSS3\n",
      "PSUM\n",
      "PSUS\n",
      "PTC\n",
      "PTHS\n",
      "PTK\n",
      "PTKDF\n",
      "PUMA\n",
      "PURL\n",
      "PUST\n",
      "PVAN\n",
      "PWCQ\n",
      "PWOR\n",
      "PWOS\n",
      "PWP09\n",
      "PWP10\n",
      "PWP11\n",
      "PWP12\n",
      "PWPN\n",
      "PWWK\n",
      "PXLN\n",
      "PXTC\n",
      "PZ1\n",
      "PZ2\n",
      "PZEN\n",
      "RAV\n",
      "REN\n",
      "RIN\n",
      "RIX\n",
      "RNA\n",
      "ROE\n",
      "RQS\n",
      "RTR\n",
      "S00\n",
      "S99\n",
      "SCG\n",
      "SHM\n",
      "SOI\n",
      "SOK\n",
      "SOM\n",
      "SS1\n",
      "STH\n",
      "SUM\n",
      "TBTH\n",
      "TD0\n",
      "TD2\n",
      "TDAG\n",
      "TFTH\n",
      "THP1\n",
      "THP2\n",
      "THP3\n",
      "THS\n",
      "TMP\n",
      "TOR\n",
      "TPR\n",
      "TSB\n",
      "TSP\n",
      "UDS\n",
      "UGIN\n",
      "UGL\n",
      "ULG\n",
      "UMA\n",
      "UNH\n",
      "USG\n",
      "UST\n",
      "V09\n",
      "V10\n",
      "V11\n",
      "V12\n",
      "V13\n",
      "V14\n",
      "V15\n",
      "V16\n",
      "V17\n",
      "VIS\n",
      "VMA\n",
      "W16\n",
      "W17\n",
      "WC00\n",
      "WC01\n",
      "WC02\n",
      "WC03\n",
      "WC04\n",
      "WC97\n",
      "WC98\n",
      "WC99\n",
      "WTH\n",
      "WWK\n",
      "XLN\n",
      "ZEN\n",
      "Processing Stage 2\n",
      "Processing Stage 3\n",
      "Processing Stage 4\n",
      "Processing Stage 5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import category_encoders as ce\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "from operator import itemgetter as itg\n",
    "\n",
    "Dict = []\n",
    "FrequencyDict = {}\n",
    "InfrequentDict = []\n",
    "DataArray = []\n",
    "TrueDict = {}\n",
    "\n",
    "with open('AllSets.json') as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "Rtext = []\n",
    "lasttext = [\"Blahblahblah\"]\n",
    "counter = 0\n",
    "Dict.append(\"<PAD>\")\n",
    "TrueDict['<PAD>'] = counter\n",
    "oglasttext = ''\n",
    "print(\"Processing Stage 1\")\n",
    "for i, dataz in enumerate(data):\n",
    "    print(dataz)\n",
    "    for z, datazz in enumerate(data[dataz]['cards']):\n",
    "        if 'text' in datazz:\n",
    "            if oglasttext != datazz['text']:\n",
    "                lasttext = datazz['text']\n",
    "                oglasttext = datazz['text']\n",
    "                lasttext = lasttext.replace(\"'\", \"\")\n",
    "                lasttext = lasttext.replace('\"', \"\")\n",
    "                lasttext = lasttext.replace(\")\", \"\")\n",
    "                lasttext = lasttext.replace(\"(\", \"\")\n",
    "                lasttext = lasttext.replace(\"]\", \"\")\n",
    "                lasttext = lasttext.replace(\"[\", \"\")\n",
    "                lasttext = lasttext.replace(\"}\", \"\")\n",
    "                lasttext = lasttext.replace(\"{\", \"\")\n",
    "                lasttext = lasttext.replace(\".\", \" <EOS> \")\n",
    "                for word in lasttext.split():\n",
    "                    if word not in FrequencyDict:\n",
    "                        FrequencyDict[word] = 1\n",
    "                    else:\n",
    "                        FrequencyDict[word] = FrequencyDict[word] + 1\n",
    "print(\"Processing Stage 2\")\n",
    "for i in FrequencyDict:\n",
    "    if FrequencyDict[i] <= 20:\n",
    "        if i.split()[0].istitle() == True:\n",
    "            InfrequentDict.append(i)\n",
    "print(\"Processing Stage 3\")\n",
    "for i, dataz in enumerate(data):\n",
    "    #print(dataz)\n",
    "    for z, datazz in enumerate(data[dataz]['cards']):\n",
    "        if 'text' in datazz:\n",
    "            if oglasttext != datazz['text']:\n",
    "                lasttext = datazz['text']\n",
    "                oglasttext = datazz['text']\n",
    "                lasttext = lasttext.replace(\"'\", \"\")\n",
    "                lasttext = lasttext.replace('\"', \"\")\n",
    "                lasttext = lasttext.replace(\")\", \"\")\n",
    "                lasttext = lasttext.replace(\"(\", \"\")\n",
    "                lasttext = lasttext.replace(\"]\", \"\")\n",
    "                lasttext = lasttext.replace(\"[\", \"\")\n",
    "                lasttext = lasttext.replace(\"}\", \"\")\n",
    "                lasttext = lasttext.replace(\"{\", \"\")\n",
    "                lasttext = lasttext.replace(\".\", \" <EOS> \")\n",
    "                for word in lasttext.split():\n",
    "                    if word in InfrequentDict:\n",
    "                        lasttext = lasttext.replace(word, \" <NAME> \")\n",
    "                DataArray.append(lasttext)\n",
    "                for word in lasttext.split():\n",
    "                    if word not in Dict:\n",
    "                        Dict.append(word)\n",
    "                        counter += 1\n",
    "                        TrueDict[word] = counter\n",
    "                        \n",
    "print(\"Processing Stage 4\")\n",
    "#Dict.append(\"<EOS>\")\n",
    "MaxLen = 0\n",
    "MaxStr = []\n",
    "batch_size = 100\n",
    "for l in range(len(DataArray)%batch_size):\n",
    "    DataArray.pop()\n",
    "for x, i in enumerate(DataArray):\n",
    "    if len(DataArray[x].split()) > MaxLen:\n",
    "        MaxLen = len(DataArray[x].split())\n",
    "        MaxStr = DataArray[x]\n",
    "print(\"Processing Stage 5\")\n",
    "BatchTensor = torch.zeros((int(len(DataArray)/batch_size),batch_size, MaxLen), device = 'cuda', dtype=torch.long) \n",
    "with torch.no_grad():\n",
    "    for z in range(int(len(DataArray)/batch_size)):\n",
    "        for x in range(batch_size):\n",
    "            for l in range(len(DataArray[z*batch_size+x].split())):\n",
    "                potato = TrueDict[DataArray[z*batch_size+x].split()[l]]\n",
    "                BatchTensor[z][x][l] = potato      \n",
    "            for b in range(MaxLen-l):\n",
    "                if b != 0:\n",
    "                    BatchTensor[z][x][l+b] = TrueDict[\"<PAD>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0368],\n",
      "         [ 1.7802],\n",
      "         [-0.0195],\n",
      "         ...,\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[ 1.0150],\n",
      "         [-0.4068],\n",
      "         [ 0.9770],\n",
      "         ...,\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[ 0.2954],\n",
      "         [-2.5393],\n",
      "         [ 1.1302],\n",
      "         ...,\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1609],\n",
      "         [-1.5069],\n",
      "         [-2.5393],\n",
      "         ...,\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-0.1565],\n",
      "         [-2.5393],\n",
      "         [ 0.2954],\n",
      "         ...,\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]],\n",
      "\n",
      "        [[-0.7049],\n",
      "         [-0.0195],\n",
      "         [-2.5393],\n",
      "         ...,\n",
      "         [ 0.0000],\n",
      "         [ 0.0000],\n",
      "         [ 0.0000]]], device='cuda:0', grad_fn=<EmbeddingBackward>)\n",
      "torch.Size([100, 444, 1])\n",
      "torch.Size([100, 444, 500])\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bryce/anaconda3/envs/pt/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3275: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "AI = MTGAI(len(TrueDict), 100)\n",
    "AI = AI.cuda()\n",
    "LossF1 = nn.CrossEntropyLoss()\n",
    "OptimF1 = optim.Adam(AI.parameters(), lr=.001)\n",
    "bestloss = 0\n",
    "epochs = 10\n",
    "lengths = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "lenph = []\n",
    "indexes = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "SortedData = torch.zeros((batch_size, MaxLen), device = 'cuda', dtype=torch.long)\n",
    "firstiter= True\n",
    "for epoch in range(epochs):\n",
    "    totalloss = 0\n",
    "    for i in range(int(len(DataArray)/batch_size)):\n",
    "        datal = []\n",
    "        AI.train()\n",
    "        hidden = AI.init_hidden(batch_size)\n",
    "        \n",
    "        \n",
    "        CardData = BatchTensor[i]\n",
    "        CardData = CardData.cuda()\n",
    "        for d in range(batch_size):\n",
    "            lenph = [g for g in CardData[d] if g!=0]\n",
    "            lengths[d] = len(lenph)\n",
    "            indexes[d] = d\n",
    "        targets = torch.zeros((batch_size, MaxLen), device = 'cuda', dtype=torch.long)\n",
    "        ### Making the targets\n",
    "        #print(lengths)\n",
    "        #print(lengths.sort(reverse=True))\n",
    "        #lengths, indexes = zip(*sorted(zip(lengths, indexes)))\n",
    "        #print(lengths)\n",
    "        #print(indexes)\n",
    "        #targets = torch.zeros(MaxLen, max(lengths))\n",
    "        #print(targets)\n",
    "        for d in range(batch_size):\n",
    "            #print(d)\n",
    "            for x in range(lengths[d]-1):\n",
    "                targets[d][x] = CardData[d][x+1]  \n",
    "                # Set targets to be the next word in sentence\n",
    "            #for z in range(MaxLen-lengths[d]-1):\n",
    "                #targets[d][lengths[d]+z] = 0\n",
    "                # Pad targets to be equal to padded data\n",
    "        #for i in range(10):\n",
    "           # print(CardData[i])    \n",
    "            #print(targets[i])\n",
    "        #datal = [(card_dataext, len([g for g in card_dataext if g!=0])) for card_dataext in CardData]\n",
    "        #datasorted = sorted(datal, key=itg(1) , reverse=True)\n",
    "        #CDS = torch.zeros((batch_size, MaxLen), device = 'cuda', dtype=torch.long)\n",
    "        #LDS = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        #for bloopy in range(batch_size):\n",
    "            #CDS[bloopy] = datasorted[bloopy][0]\n",
    "            #LDS[bloopy] = datasorted[bloopy][1]\n",
    "        #lengths.sort(reverse=True)\n",
    "            #print(lengths)\n",
    "        #print(CardData[0])\n",
    "        #print(SortedData[0])\n",
    "        \n",
    "        ### Making the targets\n",
    "            #CardData = (DataArray[i].split())[x]\n",
    "            #NextData = (DataArray[i].split())[x+1]\n",
    "            #NextData = torch.tensor(TrueDict[NextData], device='cuda')\n",
    "            #NextData = NextData.unsqueeze(0)\n",
    "        OptimF1.zero_grad()\n",
    "        predictions, hidden = AI(CardData, hidden, batch_size, False)\n",
    "        #for debug in range(10):\n",
    "            #print(predictions.view(-1, len(Dict)))\n",
    "            #print(len(predictions.view(-1, len(Dict))))\n",
    "            #sys.exit()\n",
    "            #print(len(predictions))\n",
    "            #loss = LossF1(torch.narrow(predictions.view(-1, len(Dict)), 0, 0, lengths[x]), torch.narrow(targets[x], 0, 0, lengths[x]))\n",
    "        loss = LossF1(predictions, targets)\n",
    "\n",
    "        loss.backward(retain_graph=True)\n",
    "\n",
    "        totalloss+= loss.item()\n",
    "                    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(AI.parameters(), .25)\n",
    "\n",
    "        OptimF1.step()\n",
    "        if i != 0:\n",
    "            print(\"Trained on \",i*batch_size, \" cards.\")\n",
    "            print(\"Avg loss is: \",totalloss/(i*100))\n",
    "            \n",
    "            #total_loss += len(data) * loss.data\n",
    "    print(\"Total loss this epoch:\", totalloss)\n",
    "    if bestloss > totalloss or bestloss==0:\n",
    "        bestloss = totalloss\n",
    "        print(\"New best loss! Val: \", bestloss)\n",
    "        torch.save(AI.state_dict(), \"./MTGAIbckup.model.pth\")\n",
    "\n",
    "#print(TrueDict)\n",
    "print(\"Done with training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MTGAI:\n\tsize mismatch for Embed1.weight: copying a param of torch.Size([3398, 100]) from checkpoint, where the shape is torch.Size([13301, 131]) in current model.\n\tsize mismatch for GRU.weight_ih_l0: copying a param of torch.Size([1500, 100]) from checkpoint, where the shape is torch.Size([1500, 131]) in current model.\n\tsize mismatch for Lin1.weight: copying a param of torch.Size([3398, 500]) from checkpoint, where the shape is torch.Size([13301, 500]) in current model.\n\tsize mismatch for Lin1.bias: copying a param of torch.Size([3398]) from checkpoint, where the shape is torch.Size([13301]) in current model.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cd73c2ca0ec5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mAI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMTGAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrueDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mAI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mAI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./MTGAIbckup.model.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtemperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m--> 719\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MTGAI:\n\tsize mismatch for Embed1.weight: copying a param of torch.Size([3398, 100]) from checkpoint, where the shape is torch.Size([13301, 131]) in current model.\n\tsize mismatch for GRU.weight_ih_l0: copying a param of torch.Size([1500, 100]) from checkpoint, where the shape is torch.Size([1500, 131]) in current model.\n\tsize mismatch for Lin1.weight: copying a param of torch.Size([3398, 500]) from checkpoint, where the shape is torch.Size([13301, 500]) in current model.\n\tsize mismatch for Lin1.bias: copying a param of torch.Size([3398]) from checkpoint, where the shape is torch.Size([13301]) in current model."
     ]
    }
   ],
   "source": [
    "AI = MTGAI(len(TrueDict), 100)\n",
    "AI = AI.cuda()\n",
    "AI.load_state_dict(torch.load(\"./MTGAIbckup.model.pth\"))\n",
    "hidden = AI.init_hidden(1, 1)\n",
    "temperature = 10\n",
    "num_words = 10\n",
    "initword = Dict[random.randint(0, len(Dict))]\n",
    "print(initword, '', end='')\n",
    "initword = TrueDict[initword]\n",
    "initword = torch.tensor(initword, device = 'cuda')\n",
    "\n",
    "for i in range(num_words):\n",
    "    #print(i)\n",
    "    #print(Dict[9374]\n",
    "    output, hidden = AI(initword, hidden, 1, True)\n",
    "    word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    initword.data.fill_(word_idx)\n",
    "    for searchword, searchid in TrueDict.items():\n",
    "        if searchid == word_idx:\n",
    "            word = searchword\n",
    "\n",
    "\n",
    "    print(word + ' ', end='')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTGAI(nn.Module):\n",
    "    \n",
    "    def __init__(self, Dsize, input_size):\n",
    "        super(MTGAI, self).__init__()\n",
    "        self.Embed1 = nn.Embedding(Dsize, 1, padding_idx=0)\n",
    "        self.GRU = nn.GRU(1, 500, 3, batch_first=True) # input size, hidden size, num of layers to use\n",
    "        self.Drop1 = nn.Dropout(.5)\n",
    "        self.Drop2 = nn.Dropout(.5)\n",
    "        self.Lin1 = nn.Linear(500, Dsize)\n",
    "        #self.Lin1.bias.data.fill_(0)\n",
    "        #self.Lin1.weight.data.uniform_(-.1, .1)\n",
    "        self.input_size = input_size\n",
    "        self.hidden = 0\n",
    "        \n",
    "    def forward(self, x, h, batch_size, test):\n",
    "        output = []\n",
    "        if test == False:\n",
    "            y = self.Embed1(x)\n",
    "            print(y)\n",
    "            print(y.size())\n",
    "            y, newh = self.GRU(y, h)\n",
    "            print(y.size())\n",
    "            sys.exit()\n",
    "            y = self.Lin1(y)\n",
    "            #for z in x:\n",
    "                #y = self.Embed1(z).view(1, 1, -1)\n",
    "                #y, newh = self.GRU(y, h)\n",
    "                #y = self.Lin1(y)\n",
    "                #output.append(y)\n",
    "        if test == True:\n",
    "            y = x\n",
    "            y = y.unsqueeze(0)\n",
    "            #print(y)\n",
    "            y = self.Embed1(y)\n",
    "            y = y.unsqueeze(0)\n",
    "            y, newh = self.GRU(y, h)\n",
    "            y = self.Lin1(y)\n",
    "        return y, newh\n",
    "    \n",
    "    def init_hidden(self, batch_size, input_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(3, 100, 500).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
