{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10E\n",
      "2ED\n",
      "3ED\n",
      "4ED\n",
      "5DN\n",
      "5ED\n",
      "6ED\n",
      "7ED\n",
      "8ED\n",
      "9ED\n",
      "A25\n",
      "AER\n",
      "AKH\n",
      "ALA\n",
      "ALL\n",
      "ANA\n",
      "APC\n",
      "ARB\n",
      "ARC\n",
      "ARN\n",
      "ATH\n",
      "ATQ\n",
      "AVR\n",
      "BBD\n",
      "BFZ\n",
      "BNG\n",
      "BOK\n",
      "BRB\n",
      "BTD\n",
      "C13\n",
      "C14\n",
      "C15\n",
      "C16\n",
      "C17\n",
      "C18\n",
      "CED\n",
      "CEI\n",
      "CHK\n",
      "CHR\n",
      "CM1\n",
      "CM2\n",
      "CMA\n",
      "CMD\n",
      "CN2\n",
      "CNS\n",
      "CON\n",
      "CP1\n",
      "CP2\n",
      "CP3\n",
      "CSP\n",
      "CST\n",
      "DD1\n",
      "DD2\n",
      "DDC\n",
      "DDD\n",
      "DDE\n",
      "DDF\n",
      "DDG\n",
      "DDH\n",
      "DDI\n",
      "DDJ\n",
      "DDK\n",
      "DDL\n",
      "DDM\n",
      "DDN\n",
      "DDO\n",
      "DDP\n",
      "DDQ\n",
      "DDR\n",
      "DDS\n",
      "DDT\n",
      "DDU\n",
      "DGM\n",
      "DIS\n",
      "DKA\n",
      "DKM\n",
      "DOM\n",
      "DPA\n",
      "DRB\n",
      "DRK\n",
      "DST\n",
      "DTK\n",
      "DVD\n",
      "E01\n",
      "E02\n",
      "EMA\n",
      "EMN\n",
      "EVE\n",
      "EVG\n",
      "EXO\n",
      "EXP\n",
      "F01\n",
      "F02\n",
      "F03\n",
      "F04\n",
      "F05\n",
      "F06\n",
      "F07\n",
      "F08\n",
      "F09\n",
      "F10\n",
      "F11\n",
      "F12\n",
      "F13\n",
      "F14\n",
      "F15\n",
      "F16\n",
      "F17\n",
      "F18\n",
      "FBB\n",
      "FEM\n",
      "FNM\n",
      "FRF\n",
      "FUT\n",
      "G00\n",
      "G01\n",
      "G02\n",
      "G03\n",
      "G04\n",
      "G05\n",
      "G06\n",
      "G07\n",
      "G08\n",
      "G09\n",
      "G10\n",
      "G11\n",
      "G17\n",
      "G18\n",
      "G99\n",
      "GK1\n",
      "GK2\n",
      "GNT\n",
      "GPT\n",
      "GRN\n",
      "GS1\n",
      "GTC\n",
      "GVL\n",
      "H09\n",
      "H17\n",
      "HHO\n",
      "HML\n",
      "HOP\n",
      "HOU\n",
      "HTR\n",
      "HTR17\n",
      "ICE\n",
      "IMA\n",
      "INV\n",
      "ISD\n",
      "ITP\n",
      "J12\n",
      "J13\n",
      "J14\n",
      "J15\n",
      "J16\n",
      "J17\n",
      "J18\n",
      "JGP\n",
      "JOU\n",
      "JUD\n",
      "JVC\n",
      "KLD\n",
      "KTK\n",
      "L12\n",
      "L13\n",
      "L14\n",
      "L15\n",
      "L16\n",
      "L17\n",
      "LEA\n",
      "LEB\n",
      "LEG\n",
      "LGN\n",
      "LRW\n",
      "M10\n",
      "M11\n",
      "M12\n",
      "M13\n",
      "M14\n",
      "M15\n",
      "M19\n",
      "MBS\n",
      "MD1\n",
      "ME1\n",
      "ME2\n",
      "ME3\n",
      "ME4\n",
      "MED\n",
      "MGB\n",
      "MIR\n",
      "MM2\n",
      "MM3\n",
      "MMA\n",
      "MMQ\n",
      "MOR\n",
      "MP2\n",
      "MPR\n",
      "MPS\n",
      "MRD\n",
      "NEM\n",
      "NPH\n",
      "OARC\n",
      "OC13\n",
      "OC14\n",
      "OC15\n",
      "OC16\n",
      "OC17\n",
      "OC18\n",
      "OCM1\n",
      "OCMD\n",
      "ODY\n",
      "OE01\n",
      "OGW\n",
      "OHOP\n",
      "OLGC\n",
      "ONS\n",
      "OPC2\n",
      "OPCA\n",
      "ORI\n",
      "OVNT\n",
      "P02\n",
      "P03\n",
      "P04\n",
      "P05\n",
      "P06\n",
      "P07\n",
      "P08\n",
      "P09\n",
      "P10\n",
      "P10E\n",
      "P11\n",
      "P15A\n",
      "P2HG\n",
      "PAER\n",
      "PAKH\n",
      "PAL00\n",
      "PAL01\n",
      "PAL02\n",
      "PAL03\n",
      "PAL04\n",
      "PAL05\n",
      "PAL06\n",
      "PAL99\n",
      "PALP\n",
      "PARC\n",
      "PARL\n",
      "PAVR\n",
      "PBBD\n",
      "PBFZ\n",
      "PBNG\n",
      "PBOK\n",
      "PC2\n",
      "PCA\n",
      "PCEL\n",
      "PCMD\n",
      "PCMP\n",
      "PCY\n",
      "PD2\n",
      "PD3\n",
      "PDD2\n",
      "PDGM\n",
      "PDKA\n",
      "PDOM\n",
      "PDP10\n",
      "PDP11\n",
      "PDP12\n",
      "PDP13\n",
      "PDP14\n",
      "PDRC\n",
      "PDTK\n",
      "PDTP\n",
      "PELP\n",
      "PEMN\n",
      "PF19\n",
      "PFRF\n",
      "PG07\n",
      "PG08\n",
      "PGPX\n",
      "PGRN\n",
      "PGRU\n",
      "PGTC\n",
      "PGTW\n",
      "PHEL\n",
      "PHOP\n",
      "PHOU\n",
      "PHPR\n",
      "PHUK\n",
      "PI13\n",
      "PI14\n",
      "PIDW\n",
      "PISD\n",
      "PJAS\n",
      "PJJT\n",
      "PJOU\n",
      "PJSE\n",
      "PKLD\n",
      "PKTK\n",
      "PLC\n",
      "PLGM\n",
      "PLNY\n",
      "PLPA\n",
      "PLS\n",
      "PM10\n",
      "PM11\n",
      "PM12\n",
      "PM13\n",
      "PM14\n",
      "PM15\n",
      "PM19\n",
      "PMBS\n",
      "PMEI\n",
      "PMOA\n",
      "PMPS\n",
      "PMPS06\n",
      "PMPS07\n",
      "PMPS08\n",
      "PMPS09\n",
      "PMPS10\n",
      "PMPS11\n",
      "PNAT\n",
      "PNPH\n",
      "POGW\n",
      "POR\n",
      "PORI\n",
      "PPC1\n",
      "PPOD\n",
      "PPRE\n",
      "PPRO\n",
      "PR2\n",
      "PRED\n",
      "PREL\n",
      "PRES\n",
      "PRIX\n",
      "PRM\n",
      "PRN\n",
      "PROE\n",
      "PRTR\n",
      "PRW2\n",
      "PRWK\n",
      "PS11\n",
      "PS14\n",
      "PS15\n",
      "PS16\n",
      "PS17\n",
      "PS18\n",
      "PSAL\n",
      "PSDC\n",
      "PSOI\n",
      "PSOM\n",
      "PSS1\n",
      "PSS2\n",
      "PSS3\n",
      "PSUM\n",
      "PSUS\n",
      "PTC\n",
      "PTHS\n",
      "PTK\n",
      "PTKDF\n",
      "PUMA\n",
      "PURL\n",
      "PUST\n",
      "PVAN\n",
      "PWCQ\n",
      "PWOR\n",
      "PWOS\n",
      "PWP09\n",
      "PWP10\n",
      "PWP11\n",
      "PWP12\n",
      "PWPN\n",
      "PWWK\n",
      "PXLN\n",
      "PXTC\n",
      "PZ1\n",
      "PZ2\n",
      "PZEN\n",
      "RAV\n",
      "REN\n",
      "RIN\n",
      "RIX\n",
      "RNA\n",
      "ROE\n",
      "RQS\n",
      "RTR\n",
      "S00\n",
      "S99\n",
      "SCG\n",
      "SHM\n",
      "SOI\n",
      "SOK\n",
      "SOM\n",
      "SS1\n",
      "STH\n",
      "SUM\n",
      "TBTH\n",
      "TD0\n",
      "TD2\n",
      "TDAG\n",
      "TFTH\n",
      "THP1\n",
      "THP2\n",
      "THP3\n",
      "THS\n",
      "TMP\n",
      "TOR\n",
      "TPR\n",
      "TSB\n",
      "TSP\n",
      "UDS\n",
      "UGIN\n",
      "UGL\n",
      "ULG\n",
      "UMA\n",
      "UNH\n",
      "USG\n",
      "UST\n",
      "V09\n",
      "V10\n",
      "V11\n",
      "V12\n",
      "V13\n",
      "V14\n",
      "V15\n",
      "V16\n",
      "V17\n",
      "VIS\n",
      "VMA\n",
      "W16\n",
      "W17\n",
      "WC00\n",
      "WC01\n",
      "WC02\n",
      "WC03\n",
      "WC04\n",
      "WC97\n",
      "WC98\n",
      "WC99\n",
      "WTH\n",
      "WWK\n",
      "XLN\n",
      "ZEN\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.autograd as autograd\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import category_encoders as ce\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "from operator import itemgetter as itg\n",
    "\n",
    "Dict = []\n",
    "DataArray = []\n",
    "TrueDict = {}\n",
    "\n",
    "with open('AllSets.json') as json_data:\n",
    "    data = json.load(json_data)\n",
    "\n",
    "Rtext = []\n",
    "lasttext = [\"Blahblahblah\"]\n",
    "counter = 0\n",
    "Dict.append(\"<PAD>\")\n",
    "TrueDict['<PAD>'] = counter\n",
    "oglasttext = ''\n",
    "for i, dataz in enumerate(data):\n",
    "    print(dataz)\n",
    "    for z, datazz in enumerate(data[dataz]['cards']):\n",
    "        if 'text' in datazz:\n",
    "            if oglasttext != datazz['text']:\n",
    "                lasttext = datazz['text']\n",
    "                oglasttext = datazz['text']\n",
    "                lasttext = lasttext.replace(\".\", \" <EOS> \")\n",
    "                DataArray.append(lasttext)\n",
    "                for word in lasttext.split():\n",
    "                    if word not in Dict:\n",
    "                        Dict.append(word)\n",
    "                        counter += 1\n",
    "                        TrueDict[word] = counter\n",
    "                        \n",
    "#Dict.append(\"<EOS>\")\n",
    "MaxLen = 0\n",
    "MaxStr = []\n",
    "batch_size = 100\n",
    "for l in range(len(DataArray)%batch_size):\n",
    "    DataArray.pop()\n",
    "for x, i in enumerate(DataArray):\n",
    "    if len(DataArray[x].split()) > MaxLen:\n",
    "        MaxLen = len(DataArray[x].split())\n",
    "        MaxStr = DataArray[x]\n",
    "\n",
    "BatchTensor = torch.zeros((int(len(DataArray)/batch_size),batch_size, MaxLen), device = 'cuda', dtype=torch.long) \n",
    "with torch.no_grad():\n",
    "    for z in range(int(len(DataArray)/batch_size)):\n",
    "        for x in range(batch_size):\n",
    "            for l in range(len(DataArray[z*batch_size+x].split())):\n",
    "                potato = TrueDict[DataArray[z*batch_size+x].split()[l]]\n",
    "                BatchTensor[z][x][l] = potato      \n",
    "            for b in range(MaxLen-l):\n",
    "                if b != 0:\n",
    "                    BatchTensor[z][x][l+b] = TrueDict[\"<PAD>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4286, -0.7949,  0.3517,  ..., -0.5257, -0.1078,  0.8982],\n",
      "        [-0.0433,  0.1902, -1.0633,  ..., -1.8799, -0.6838,  1.2021],\n",
      "        [-0.0433,  0.1902, -1.0633,  ..., -1.8799, -0.6838,  1.2021],\n",
      "        ...,\n",
      "        [ 1.0639,  0.0804,  1.5272,  ..., -1.9051, -0.5459,  0.3006],\n",
      "        [-0.9518,  0.2140,  1.9428,  ...,  1.3468, -0.2104,  0.7075],\n",
      "        [ 0.6600, -1.3166,  1.5732,  ..., -0.0047, -2.2218,  1.5437]],\n",
      "       device='cuda:0', grad_fn=<PackPaddedBackward>)\n",
      "tensor([-0.4286, -0.7949,  0.3517,  0.2245, -0.1601, -0.1011,  0.7017, -0.0980,\n",
      "        -0.2014, -0.8656, -0.2535, -0.1041,  0.1264,  0.3572,  0.0502, -0.6188,\n",
      "         1.0621,  1.0926,  0.5233, -0.2034, -0.8984, -1.8276, -1.9780, -1.0054,\n",
      "        -0.8014, -2.1059,  0.2597,  0.0887, -0.3629, -0.6362, -1.0069,  0.8111,\n",
      "        -0.6247, -1.5086, -1.5764,  0.9846,  0.4699, -0.2561,  0.8728, -0.0098,\n",
      "         1.2481, -0.8217,  0.3165, -0.4868, -0.8793,  0.8314, -0.5173, -0.6089,\n",
      "         1.6284, -0.7173, -1.2669,  0.9177, -0.7832, -0.3722,  0.6981,  1.3108,\n",
      "         0.5190,  0.0337, -1.6605,  2.3683,  1.1837,  0.2918, -0.3427, -0.3060,\n",
      "        -0.4960, -0.1067,  2.3193,  0.5471, -0.5382,  0.5791,  0.2442, -1.1891,\n",
      "        -0.4330,  1.9980,  1.0515,  0.8510,  0.4091, -0.6225, -1.2368, -0.9205,\n",
      "         1.1376, -0.9238, -0.1316,  0.2188, -0.7926, -0.3996,  0.2999,  0.9733,\n",
      "         1.3458, -0.5132,  1.4900, -1.2522, -2.4822, -0.4056,  2.2034, -0.6475,\n",
      "         1.2002, -0.3563, -0.9936,  0.4501,  0.7944, -1.1102,  0.0393,  0.6355,\n",
      "        -0.5424, -0.6739, -0.7955,  0.9373,  1.2774,  1.7800,  0.3777,  0.7370,\n",
      "         0.8696, -0.9290, -0.6270,  0.1361,  0.6762, -0.7403,  0.1279, -2.3583,\n",
      "        -0.4970,  2.4164, -0.3575, -0.5278,  0.2150, -1.8430, -0.1624, -0.3324,\n",
      "        -0.5257, -0.1078,  0.8982], device='cuda:0', grad_fn=<SelectBackward>)\n",
      "1574\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected target size (100, 13301), got torch.Size([100, 131])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-44957c061731>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m#print(predictions[debug])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;31m#print(len(predictions))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLossF1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;31m#loss = LossF1(predictions.view(-1, len(Dict)), targets)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    860\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 862\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1548\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1549\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1550\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   1414\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[0;32m-> 1416\u001b[0;31m                 out_size, target.size()))\n\u001b[0m\u001b[1;32m   1417\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected target size (100, 13301), got torch.Size([100, 131])"
     ]
    }
   ],
   "source": [
    "\n",
    "AI = MTGAI(len(TrueDict), MaxLen)\n",
    "AI = AI.cuda()\n",
    "LossF1 = nn.CrossEntropyLoss()\n",
    "OptimF1 = optim.Adam(AI.parameters(), lr=.001)\n",
    "bestloss = 0\n",
    "epochs = 10\n",
    "lengths = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "lenph = []\n",
    "indexes = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "SortedData = torch.zeros((batch_size, MaxLen), device = 'cuda', dtype=torch.long)\n",
    "for epoch in range(epochs):\n",
    "    totalloss = 0\n",
    "    for i in range(int(len(DataArray)/batch_size)):\n",
    "        datal = []\n",
    "        AI.train()\n",
    "        hidden = AI.init_hidden(batch_size)\n",
    "        \n",
    "        \n",
    "        CardData = BatchTensor[i]\n",
    "        for d in range(batch_size):\n",
    "            lenph = [g for g in CardData[d] if g!=0]\n",
    "            lengths[d] = len(lenph)\n",
    "            indexes[d] = d\n",
    "        targets = torch.zeros((batch_size, MaxLen), device = 'cuda', dtype=torch.long)\n",
    "        ### Making the targets\n",
    "        #print(lengths)\n",
    "        #print(lengths.sort(reverse=True))\n",
    "        #lengths, indexes = zip(*sorted(zip(lengths, indexes)))\n",
    "        #print(lengths)\n",
    "        #print(indexes)\n",
    "        #targets = torch.zeros(MaxLen, max(lengths))\n",
    "        #print(targets)\n",
    "        for d in range(batch_size):\n",
    "            #print(d)\n",
    "            for x in range(lengths[d]-1):\n",
    "                targets[d][x] = CardData[d][x+1]  \n",
    "                # Set targets to be the next word in sentence\n",
    "            for z in range(MaxLen-lengths[d]-1):\n",
    "                targets[d][lengths[d]+z] = 0\n",
    "                # Pad targets to be equal to padded data\n",
    "        #for i in range(10):\n",
    "           # print(CardData[i])    \n",
    "            #print(targets[i])\n",
    "        datal = [(card_dataext, len([g for g in card_dataext if g!=0])) for card_dataext in CardData]\n",
    "        datasorted = sorted(datal, key=itg(1) , reverse=True)\n",
    "        CDS = torch.zeros((batch_size, MaxLen), device = 'cuda', dtype=torch.long)\n",
    "        LDS = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "        for bloopy in range(batch_size):\n",
    "            CDS[bloopy] = datasorted[bloopy][0]\n",
    "            LDS[bloopy] = datasorted[bloopy][1]\n",
    "        #lengths.sort(reverse=True)\n",
    "            #print(lengths)\n",
    "        #print(CardData[0])\n",
    "        #print(SortedData[0])\n",
    "        \n",
    "        ### Making the targets\n",
    "            #CardData = (DataArray[i].split())[x]\n",
    "            #NextData = (DataArray[i].split())[x+1]\n",
    "            #NextData = torch.tensor(TrueDict[NextData], device='cuda')\n",
    "            #NextData = NextData.unsqueeze(0)\n",
    "        OptimF1.zero_grad()\n",
    "        predictions, hidden = AI(CDS, hidden, LDS, batch_size)\n",
    "        #for debug in range(10):\n",
    "            #print(predictions[debug])\n",
    "            #print(len(predictions))\n",
    "        loss = LossF1(predictions, targets)\n",
    "        #loss = LossF1(predictions.view(-1, len(Dict)), targets)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        print(loss)\n",
    "            #loss.backward(retain_graph=True)\n",
    "        totalloss+= loss.item()\n",
    "                    # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        torch.nn.utils.clip_grad_norm_(AI.parameters(), .25)\n",
    "\n",
    "        OptimF1.step()\n",
    "        sys.exit()\n",
    "        if i % 10 == 0:\n",
    "            print(\"Trained on \",i*batch_size, \" cards.\")\n",
    "            \n",
    "            #total_loss += len(data) * loss.data\n",
    "    print(\"Total loss this epoch:\", totalloss)\n",
    "    if bestloss > totalloss or bestloss==0:\n",
    "        bestloss = totalloss\n",
    "        print(\"New best loss! Val: \", bestloss)\n",
    "        torch.save(AI.state_dict(), \"./MTGAIbckup.model.pth\")\n",
    "\n",
    "#print(TrueDict)\n",
    "print(\"Done with training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AI.load_state_dict(torch.load(\"./MTGAIbckup.model.pth\"))\n",
    "hidden = AI.init_hidden(1)\n",
    "temperature = .5\n",
    "num_words = 10\n",
    "initword = Dict[random.randint(0, len(Dict))]\n",
    "print(initword, '', end='')\n",
    "initword = TrueDict[initword]\n",
    "initword = torch.tensor(initword)\n",
    "\n",
    "for i in range(num_words):\n",
    "    #print(i)\n",
    "    #print(Dict[9374])\n",
    "    output, hidden = AI(Dict[initword], hidden)\n",
    "    word_weights = output.squeeze().data.div(temperature).exp().cpu()\n",
    "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "    initword.data.fill_(word_idx)\n",
    "    for searchword, searchid in TrueDict.items():\n",
    "        if searchid == word_idx:\n",
    "            word = searchword\n",
    "\n",
    "\n",
    "    print(word + ' ', end='')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTGAI(nn.Module):\n",
    "    \n",
    "    def __init__(self, Dsize, input_size):\n",
    "        super(MTGAI, self).__init__()\n",
    "        self.Embed1 = nn.Embedding(Dsize, input_size, padding_idx=0)\n",
    "        self.GRU = nn.GRU(input_size, 200, 1) # input size, hidden size, num of layers to use\n",
    "        self.Drop1 = nn.Dropout(.5)\n",
    "        self.Drop2 = nn.Dropout(.5)\n",
    "        self.Lin1 = nn.Linear(200, Dsize)\n",
    "        self.Lin1.bias.data.fill_(0)\n",
    "        self.Lin1.weight.data.uniform_(-.1, .1)\n",
    "        self.input_size = input_size\n",
    "        self.hidden = 0\n",
    "        \n",
    "    def forward(self, x, h, Xlen, batch_size):\n",
    "        #with torch.no_grad():\n",
    "            #x = TrueDict[x]\n",
    "            #x = torch.tensor(x, device = 'cuda')\n",
    "            #x = x.unsqueeze(0)\n",
    "            #x = x.unsqueeze(0)\n",
    "        x = self.Embed1(x)\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, Xlen, batch_first=True)\n",
    "        x, h = self.GRU(x, h)\n",
    "        x, xlen = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
    "        #x = x.contiguous()\n",
    "        #x = x.view(batch_size, x.shape[2]*x.shape[1])\n",
    "        x = self.Lin1(x)\n",
    "        return x, h\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return Variable(weight.new(1, batch_size, 200).zero_())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
